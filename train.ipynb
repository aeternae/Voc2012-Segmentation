{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "begin:\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0805 10:04:34.349962 139950820947776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0805 10:04:34.366004 139950820947776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "(?, 256, 256, 3)\n",
      "W0805 10:04:34.379450 139950820947776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0805 10:04:34.379588 139950820947776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0805 10:04:34.379716 139950820947776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2019-08-05 10:04:34.379915: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-08-05 10:04:34.389541: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2019-08-05 10:04:34.733759: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5b03fc0 executing computations on platform CUDA. Devices:\n",
      "2019-08-05 10:04:34.733796: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2019-08-05 10:04:34.755321: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400090000 Hz\n",
      "2019-08-05 10:04:34.759743: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c5f8b0 executing computations on platform Host. Devices:\n",
      "2019-08-05 10:04:34.759773: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-08-05 10:04:34.762653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
      "pciBusID: 0000:0e:00.0\n",
      "2019-08-05 10:04:34.763005: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-08-05 10:04:34.764583: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-08-05 10:04:34.766031: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-08-05 10:04:34.766364: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-08-05 10:04:34.768246: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-08-05 10:04:34.769725: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-08-05 10:04:34.774057: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-08-05 10:04:34.784915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2019-08-05 10:04:34.785010: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-08-05 10:04:34.789518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-08-05 10:04:34.789550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2019-08-05 10:04:34.789565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2019-08-05 10:04:34.797129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:0e:00.0, compute capability: 7.5)\n",
      "W0805 10:04:35.742496 139950820947776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0805 10:04:35.964355 139950820947776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "(?, 128, 128, 64)\n",
      "(?, 128, 128, 64)\n",
      "(?, 64, 64, 128)\n",
      "(?, 64, 64, 128)\n",
      "(?, 32, 32, 256)\n",
      "(?, 32, 32, 256)\n",
      "(?, 16, 16, 512)\n",
      "(?, 16, 16, 512)\n",
      "(?, 16, 16, 1024)\n",
      "attention_block:\n",
      "(?, 16, 16, 256)\n",
      "(?, 16, 16, 256)\n",
      "(?, 16, 16, 256)\n",
      "(?, 16, 16, 1)\n",
      "(?, 16, 16, 1)\n",
      "W0805 10:04:36.729725 139950820947776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "(?, 32, 32, 1)\n",
      "(?, 32, 32, 512)\n",
      "-----------------\n",
      "(?, 32, 32, 512)\n",
      "(?, 32, 32, 512)\n",
      "attention_block:\n",
      "(?, 32, 32, 128)\n",
      "(?, 32, 32, 128)\n",
      "(?, 32, 32, 128)\n",
      "(?, 32, 32, 1)\n",
      "(?, 32, 32, 1)\n",
      "(?, 64, 64, 1)\n",
      "(?, 64, 64, 256)\n",
      "-----------------\n",
      "(?, 64, 64, 256)\n",
      "(?, 64, 64, 256)\n",
      "attention_block:\n",
      "(?, 64, 64, 64)\n",
      "(?, 64, 64, 64)\n",
      "(?, 64, 64, 64)\n",
      "(?, 64, 64, 1)\n",
      "(?, 64, 64, 1)\n",
      "(?, 128, 128, 1)\n",
      "(?, 128, 128, 128)\n",
      "-----------------\n",
      "(?, 128, 128, 128)\n",
      "(?, 128, 128, 128)\n",
      "attention_block:\n",
      "(?, 128, 128, 32)\n",
      "(?, 128, 128, 32)\n",
      "(?, 128, 128, 32)\n",
      "(?, 128, 128, 1)\n",
      "(?, 128, 128, 1)\n",
      "(?, 256, 256, 1)\n",
      "(?, 256, 256, 64)\n",
      "-----------------\n",
      "(?, 256, 256, 64)\n",
      "(?, 256, 256, 64)\n",
      "(?, 256, 256, 21)\n",
      "(?, 256, 256, 21)\n",
      "W0805 10:04:37.707553 139950820947776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "1464\n",
      "1449\n",
      "W0805 10:04:45.347171 139950820947776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0805 10:04:45.347774 139950820947776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/100\n",
      "2019-08-05 10:04:50.311084: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.0438 - MIoU: 0.0438next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.0507 - MIoU: 0.0507next epoch\n",
      "183/183 [==============================] - 180s 982ms/step - loss: -0.0514 - MIoU: 0.0514 - val_loss: -0.2479 - val_MIoU: 0.2479\n",
      "Epoch 2/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.3298 - MIoU: 0.3298next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.3330 - MIoU: 0.3330next epoch\n",
      "183/183 [==============================] - 169s 923ms/step - loss: -0.3335 - MIoU: 0.3335 - val_loss: -0.4249 - val_MIoU: 0.4249\n",
      "Epoch 3/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.4594 - MIoU: 0.4594next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.4599 - MIoU: 0.4599next epoch\n",
      "183/183 [==============================] - 170s 930ms/step - loss: -0.4600 - MIoU: 0.4600 - val_loss: -0.4793 - val_MIoU: 0.4793\n",
      "Epoch 4/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.5158 - MIoU: 0.5158next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5153 - MIoU: 0.5153next epoch\n",
      "183/183 [==============================] - 168s 915ms/step - loss: -0.5153 - MIoU: 0.5153 - val_loss: -0.5193 - val_MIoU: 0.5193\n",
      "Epoch 5/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.5418 - MIoU: 0.5418next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5407 - MIoU: 0.5407next epoch\n",
      "183/183 [==============================] - 166s 906ms/step - loss: -0.5407 - MIoU: 0.5407 - val_loss: -0.5367 - val_MIoU: 0.5367\n",
      "Epoch 6/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.5544 - MIoU: 0.5544next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5533 - MIoU: 0.5533next epoch\n",
      "183/183 [==============================] - 165s 903ms/step - loss: -0.5532 - MIoU: 0.5532 - val_loss: -0.5480 - val_MIoU: 0.5480\n",
      "Epoch 7/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.5634 - MIoU: 0.5634next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5621 - MIoU: 0.5621next epoch\n",
      "183/183 [==============================] - 166s 908ms/step - loss: -0.5620 - MIoU: 0.5620 - val_loss: -0.5564 - val_MIoU: 0.5564\n",
      "Epoch 8/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.5706 - MIoU: 0.5706next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5692 - MIoU: 0.5692next epoch\n",
      "183/183 [==============================] - 166s 907ms/step - loss: -0.5691 - MIoU: 0.5691 - val_loss: -0.5648 - val_MIoU: 0.5648\n",
      "Epoch 9/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.5756 - MIoU: 0.5756next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5742 - MIoU: 0.5742next epoch\n",
      "183/183 [==============================] - 166s 905ms/step - loss: -0.5741 - MIoU: 0.5741 - val_loss: -0.5709 - val_MIoU: 0.5709\n",
      "Epoch 10/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.5794 - MIoU: 0.5794next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5780 - MIoU: 0.5780next epoch\n",
      "183/183 [==============================] - 166s 909ms/step - loss: -0.5779 - MIoU: 0.5779 - val_loss: -0.5755 - val_MIoU: 0.5755\n",
      "Epoch 11/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.5826 - MIoU: 0.5826next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5812 - MIoU: 0.5812next epoch\n",
      "183/183 [==============================] - 166s 907ms/step - loss: -0.5811 - MIoU: 0.5811 - val_loss: -0.5789 - val_MIoU: 0.5789\n",
      "Epoch 12/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.5853 - MIoU: 0.5853next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5838 - MIoU: 0.5838next epoch\n",
      "183/183 [==============================] - 167s 911ms/step - loss: -0.5837 - MIoU: 0.5837 - val_loss: -0.5817 - val_MIoU: 0.5817\n",
      "Epoch 13/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.5876 - MIoU: 0.5876next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5861 - MIoU: 0.5861next epoch\n",
      "183/183 [==============================] - 165s 904ms/step - loss: -0.5859 - MIoU: 0.5859 - val_loss: -0.5843 - val_MIoU: 0.5843\n",
      "Epoch 14/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.5896 - MIoU: 0.5896next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5881 - MIoU: 0.5881next epoch\n",
      "183/183 [==============================] - 167s 910ms/step - loss: -0.5880 - MIoU: 0.5880 - val_loss: -0.5864 - val_MIoU: 0.5864\n",
      "Epoch 15/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.5914 - MIoU: 0.5914next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5899 - MIoU: 0.5899next epoch\n",
      "183/183 [==============================] - 166s 906ms/step - loss: -0.5898 - MIoU: 0.5898 - val_loss: -0.5884 - val_MIoU: 0.5884\n",
      "Epoch 16/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.5933 - MIoU: 0.5933next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5918 - MIoU: 0.5918next epoch\n",
      "183/183 [==============================] - 165s 901ms/step - loss: -0.5916 - MIoU: 0.5916 - val_loss: -0.5899 - val_MIoU: 0.5899\n",
      "Epoch 17/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.5953 - MIoU: 0.5953next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5938 - MIoU: 0.5938next epoch\n",
      "183/183 [==============================] - 166s 907ms/step - loss: -0.5936 - MIoU: 0.5936 - val_loss: -0.5935 - val_MIoU: 0.5935\n",
      "Epoch 18/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.5971 - MIoU: 0.5971next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5956 - MIoU: 0.5956next epoch\n",
      "183/183 [==============================] - 165s 901ms/step - loss: -0.5955 - MIoU: 0.5955 - val_loss: -0.5949 - val_MIoU: 0.5949\n",
      "Epoch 19/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.5987 - MIoU: 0.5987next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5972 - MIoU: 0.5972next epoch\n",
      "183/183 [==============================] - 166s 910ms/step - loss: -0.5970 - MIoU: 0.5970 - val_loss: -0.5952 - val_MIoU: 0.5952\n",
      "Epoch 20/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6002 - MIoU: 0.6002next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.5987 - MIoU: 0.5987next epoch\n",
      "183/183 [==============================] - 165s 902ms/step - loss: -0.5986 - MIoU: 0.5986 - val_loss: -0.5967 - val_MIoU: 0.5967\n",
      "Epoch 21/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6035 - MIoU: 0.6035next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6020 - MIoU: 0.6020next epoch\n",
      "183/183 [==============================] - 164s 897ms/step - loss: -0.6019 - MIoU: 0.6019 - val_loss: -0.6009 - val_MIoU: 0.6009\n",
      "Epoch 22/100\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6030 - MIoU: 0.6030next epoch\n",
      "next epoch\n",
      "183/183 [==============================] - 184s 1s/step - loss: -0.6029 - MIoU: 0.6029 - val_loss: -0.6019 - val_MIoU: 0.6019\n",
      "Epoch 23/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6051 - MIoU: 0.6051next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6035 - MIoU: 0.6035next epoch\n",
      "183/183 [==============================] - 165s 903ms/step - loss: -0.6034 - MIoU: 0.6034 - val_loss: -0.6021 - val_MIoU: 0.6021\n",
      "Epoch 24/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6053 - MIoU: 0.6053next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6038 - MIoU: 0.6038next epoch\n",
      "183/183 [==============================] - 164s 898ms/step - loss: -0.6037 - MIoU: 0.6037 - val_loss: -0.6022 - val_MIoU: 0.6022\n",
      "Epoch 25/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6055 - MIoU: 0.6055next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6039 - MIoU: 0.6039next epoch\n",
      "183/183 [==============================] - 165s 904ms/step - loss: -0.6038 - MIoU: 0.6038 - val_loss: -0.6022 - val_MIoU: 0.6022\n",
      "Epoch 26/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6056 - MIoU: 0.6056next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6040 - MIoU: 0.6040next epoch\n",
      "183/183 [==============================] - 166s 905ms/step - loss: -0.6039 - MIoU: 0.6039 - val_loss: -0.6023 - val_MIoU: 0.6023\n",
      "Epoch 27/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6056 - MIoU: 0.6056next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6040 - MIoU: 0.6040next epoch\n",
      "183/183 [==============================] - 164s 896ms/step - loss: -0.6039 - MIoU: 0.6039 - val_loss: -0.6023 - val_MIoU: 0.6023\n",
      "Epoch 28/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6057 - MIoU: 0.6057next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6041 - MIoU: 0.6041next epoch\n",
      "183/183 [==============================] - 166s 909ms/step - loss: -0.6040 - MIoU: 0.6040 - val_loss: -0.6023 - val_MIoU: 0.6023\n",
      "Epoch 29/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6057 - MIoU: 0.6057next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6041 - MIoU: 0.6041next epoch\n",
      "183/183 [==============================] - 165s 902ms/step - loss: -0.6040 - MIoU: 0.6040 - val_loss: -0.6023 - val_MIoU: 0.6023\n",
      "Epoch 30/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6057 - MIoU: 0.6057next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6041 - MIoU: 0.6041next epoch\n",
      "183/183 [==============================] - 166s 904ms/step - loss: -0.6040 - MIoU: 0.6040 - val_loss: -0.6023 - val_MIoU: 0.6023\n",
      "Epoch 31/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6057 - MIoU: 0.6057next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6041 - MIoU: 0.6041next epoch\n",
      "183/183 [==============================] - 165s 901ms/step - loss: -0.6040 - MIoU: 0.6040 - val_loss: -0.6023 - val_MIoU: 0.6023\n",
      "Epoch 32/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6057 - MIoU: 0.6057next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 895ms/step - loss: -0.6040 - MIoU: 0.6040 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 33/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 166s 908ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 34/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 166s 909ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 35/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 898ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 36/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 166s 905ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 37/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 166s 908ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 38/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 899ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 39/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 160s 874ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 40/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 903ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 41/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 899ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 42/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 900ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 43/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 895ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 44/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 896ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 45/100\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "next epoch\n",
      "183/183 [==============================] - 232s 1s/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 46/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 163s 888ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 47/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 895ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 48/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 161s 878ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 49/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 159s 868ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 50/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 163s 890ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "Epoch 51/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 162s 885ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 52/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 162s 885ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 53/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 903ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 54/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 168s 920ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 55/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 894ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 56/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 167s 910ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 57/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 904ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 58/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 167s 911ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 59/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 170s 928ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 60/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 167s 910ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 61/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 169s 926ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 62/100\n",
      "172/183 [===========================>..] - ETA: 6s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 170s 931ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 63/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 167s 914ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 64/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 166s 907ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 65/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 167s 911ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 66/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 904ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 67/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 902ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 68/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 898ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 69/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 899ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 70/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 904ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 71/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 903ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 72/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 900ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 73/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 902ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 74/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 900ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 75/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 898ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 76/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 902ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 77/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 896ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 78/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 902ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 79/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 166s 904ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 80/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 894ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 81/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 897ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 82/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 899ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 83/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 165s 899ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 84/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 164s 896ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 85/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 166s 904ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 86/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 167s 910ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 87/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 166s 908ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 88/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 163s 890ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 89/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 159s 869ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 90/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 158s 861ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "Epoch 91/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 159s 867ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 92/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 159s 866ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 93/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 159s 868ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 94/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 159s 871ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 95/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 160s 872ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 96/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 159s 867ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 97/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 160s 875ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 98/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 159s 870ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 99/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 159s 871ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n",
      "Epoch 100/100\n",
      "172/183 [===========================>..] - ETA: 5s - loss: -0.6058 - MIoU: 0.6058next epoch\n",
      "182/183 [============================>.] - ETA: 0s - loss: -0.6042 - MIoU: 0.6042next epoch\n",
      "183/183 [==============================] - 159s 871ms/step - loss: -0.6041 - MIoU: 0.6041 - val_loss: -0.6024 - val_MIoU: 0.6024\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
